# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OiaXTHCUyxob41oVnZJb9LFFTqaQLAfq
"""

import torch
import torch.nn as nn

class generator(nn.Module):
  
  def __init__(
      self,
      encoding_dims = 100,
      out_size = 32,
      out_channels = 3,
      step_channels = 64,
      nonlinearity = None,
      last_nonlinearity = None
  ):
    super().__init__()
    
    if out_size < 16 or ceil(log2(out_size)) != log2(out_size):
            raise Exception(
                "Target Image Size must be at least 16*16 and an exact power of 2"
            )
        
    num_repeats = out_size.bit_length() - 4
    d = (step_channels*(2**num_repeats))
    self.encoding_dims = encoding_dims

    nl = nn.LeakyReLU(.2) if nonlinearity is None else nonlinearity
    last_nl = nn.nn.Tanh() if last_nonlinearity is None else last_nonlinearity
    
    model =[]
    
    model.append(
        nn.Sequential(
          nn.ConvTranspose2d(encoding_dims, d, 4, 1, 0),
          BatchNorm2d(d),
          nl
        )
    )
    
    for i in range(num_repeats):
      model.append(
          nn.Sequential(
          nn.nn.ConvTranspose2d(d, d//2, 4, 2, 1),
          nn.BatchNorm2d(d//2),
          nl
          )
      )
      
      d = d//2
    
    
    model.append(
        nn.Sequential(
            nn.ConvTranspose2d(step_channels, out_size, 4, 2, 1, bias = True),
            last_nl
        )
    )
    
    self.model = nn.nn.Sequential(*model)
    self.weight_initializer()
    
  def forward(self,x):
    
    x = x.view(-1, self.encoding_dims, 1, 1)
    return self.model(x)
  
  
  def _weight_initializer(self):
        r"""Default weight initializer for all generator models.
        Models that require custom weight initialization can override this method
        """
        for m in self.modules():
            if isinstance(m, nn.ConvTranspose2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)

class discriminator(nn.Module):
  
  def __init__(
      self,
      num_classes,
      in_size = 32,
      in_channels = 3,
      step_channels = 64,
      nonlinearity = None,
      last_nonlonearity = None
  ):
    
    super().__init__()
    
    if in_size < 16 or ceil(log2(in_size)) != log2(in_size):
            raise Exception(
                "Input Image Size must be at least 16*16 and an exact power of 2"
            )
    
    self.num_classes = num_classes
    num_repeats = in_size.bit_length() - 4
    d = step_channels 
    model = []
    
    nl = nn.LeakyReLU(.2) if nonlinearity is None else nonlinearity
    last_nl = nn.LeakyReLU(.2) if last_nonlinearity is None else last_nonlinearity
    
    model.append(
        nn.Sequential(
            nn.Conv2d(in_channels, d, 4, 2, 1, bias = True),
            nl)
    )
        
    for i in range(num_repeats):
        
      model.append(
          nn.Sequential(
              nn.Conv2d(d, d*2, 4, 2, 1, bias = False),
              nn.BatchNorm2d(d*2),
              nl
          )
      )
        
      d = d*2
        
  
      self.disc = nn.Sequential(
          nn.Conv2d(d, 1, 4, 1, 0, bias = True),
          nn.Sigmoid()
      )
        
      self.aux = nn.Sequential(
          nn.Conv2d(d, self.num_classes, 4, 1, 0, bias = True),
          last_nl
      )
      
      
      self.model = nn.Sequential(*model)
      self._weight_initializer()
          
  
  def forward(self, x):
        
        x = self.model(x)
        dx = self.disc(x)
        ax = self.aux(x)
        
        return dx.view(-1), ax.view(-1, self.num_classes) 
        
  def _weight_initializer(self):
        r"""Default weight initializer for all disciminator models.
        Models that require custom weight initialization can override this method
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)

